---
title: "Repo-level associations"
author: "Pamela Russell"
date: "12/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r env_setup}
rm(list=ls())
suppressPackageStartupMessages(library(bigrquery))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
setwd("~/Documents/Github_mining/src/R/exploratory_analysis")
source("project_info.R")
```

```{r util}
# Helpful functions / utilities

# Get formatted table ID e.g. "[github-bioinformatics-171721:analysis.lines_of_code_by_file]"
table_str <- function(dataset, table) {paste("[", proj, ":", dataset, ".", table, "]", sep="")}

# Get query result from BigQuery
fetch_query_res <- function(query) {query_exec(query, project = proj, max_pages = Inf)}

# Join a table to repo_data
join_tbl <- function(data) {
  repo_data <<- full_join(repo_data, data, by = "repo_name")
}

# Change NA to 0
na_to_zero <- function(x) {
  if(is.na(x)) 0
  else x
}
```

```{r langs_to_use}
# Top languages to include as covariates
# Languages included in at least 50 repos
top_langs <- c("Python",
               "Bourne Shell",
               "R",
               "C/C++ Header",
               "Perl",
               "C++",
               "JavaScript",
               "C",
               "Java",
               "Bourne Again Shell",
               "MATLAB",
               "m4",
               "SQL",
               "Ruby",
               "PHP")

# "Languages" that are actually data file types or web pages
non_lang_file_types <- c("HTML",
                         "JSON",
                         "XML",
                         "CSS",
                         "YAML",
                         "Markdown",
                         "XSD",
                         "DTD",
                         "XSLT",
                         "XMI",
                         "Haml",
                         "XHTML",
                         "XAML")
```

```{r init_df}
# Initialize table for all repo-level data
repo_data <- data.frame(repo_name = character(0))
```

```{r licenses}
join_tbl(
  list_tabledata(project = proj, dataset = ds_gh, table = table_licenses) %>%
    select(repo_name, license)
)
```

```{r proj_duration}
join_tbl(
  list_tabledata(project = proj, dataset = ds_analysis, table = table_proj_duration)
)
```

```{r num_devs}
join_tbl(
  list_tabledata(project = proj, dataset = ds_analysis, table = table_num_devs_by_repo)
)
```

```{r repo_metrics}
join_tbl(
  list_tabledata(project = proj, dataset = ds_gh, table = table_repo_metrics) %>%
    select(repo_name, is_fork, stargazers_count, watchers_count, forks_count, subscribers_count)
)
```

```{r repo_and_article}
query <- paste("SELECT repo_name, journal, year, use_repo FROM [", 
               proj, ":", ds_lit_search, ".", table_repo_and_article, "]", 
               sep="")
join_tbl(
  query_exec(query, project = proj) %>%
    filter(use_repo == T) %>%
    select(repo_name, journal, year_published = year)
)
```

```{r file_info}
query <- paste("SELECT
  file_info.repo_name AS repo_name,
  loc.language AS language,
  loc.code AS lines_of_code,
  file_info.size AS size
FROM (
  SELECT
    repo_name,
    sha
  FROM
    ", table_str(ds_gh, table_file_info), "
  GROUP BY
    repo_name,
    sha,
    size) AS file_info
INNER JOIN (
  SELECT
    sha,
    language,
    code
  FROM
    ", table_str(ds_analysis, table_loc), "
  GROUP BY
    sha,
    language,
    code) AS loc
ON
  file_info.sha = loc.sha", sep="")

file_info <- query_exec(query, project = proj, max_pages = Inf)
file_info_no_data <- file_info %>% filter(!language %in% non_lang_file_types)

join_tbl(
  file_info %>% 
    group_by(repo_name) %>% 
    summarize(
      num_langs = length(unique(language)),
      total_files = n(), 
      total_lines = sum(as.numeric(lines_of_code)),
      total_file_size = sum(as.numeric(size)), 
      largest_file_size = max(as.numeric(size)), 
      mean_file_size = mean(as.numeric(size)))
)

join_tbl(
  file_info_no_data %>% 
    group_by(repo_name) %>% 
    summarize(
      num_langs_no_data = length(unique(language)),
      total_files_no_data = n(), 
      total_lines_no_data = sum(as.numeric(lines_of_code)),
      total_file_size_no_data = sum(as.numeric(size)), 
      largest_file_size_no_data = max(as.numeric(size)), 
      mean_file_size_no_data = mean(as.numeric(size)))
)

```

```{r commit_types}
# Add data from commit types table
commit_types <- list_tabledata(project = proj, dataset = ds_analysis, table = table_commit_types) 
commit_types <- commit_types %>%
  mutate(num_bug_fix_commits = sapply(commit_types$num_bug_fix_commits, na_to_zero)) %>%
  mutate(pct_bug_fix_commits = num_bug_fix_commits / num_commits)
join_tbl(commit_types)
rm(commit_types)
```

```{r code_chunk_freq}
# Function to process one version of code chunk frequency table
process_code_chunk_freq <- function(table_name, col_suffix) {
  
  query <- paste("SELECT repo_name, num_occurrences FROM [", 
                 proj, ":", ds_analysis, ".", table_name, "]", 
                 sep = "")
  
  code_chunk_freq <- query_exec(query, project = proj, max_pages = Inf)
  
  total_code_chunks <- code_chunk_freq %>% 
    group_by(repo_name) %>% 
    summarize(total_code_chunks = n())
  
  repeated_code_chunks <- code_chunk_freq %>% 
    filter(num_occurrences > 1) %>%
    group_by(repo_name) %>% 
    summarize(repeated_code_chunks = n())
  
  max_code_chunk_occurrence <- code_chunk_freq %>%
    group_by(repo_name) %>%
    summarise(max_code_chunk_occurrence = max(num_occurrences))
  
  mean_code_chunk_occurrence <- code_chunk_freq %>%
    group_by(repo_name) %>%
    summarise(mean_code_chunk_occurrence = mean(num_occurrences))
  
  join_data <- total_code_chunks %>%
    full_join(repeated_code_chunks, by = "repo_name") %>%
    full_join(max_code_chunk_occurrence, by = "repo_name") %>%
    full_join(mean_code_chunk_occurrence, by = "repo_name")
  
  colnames(join_data) <- sapply(colnames(join_data), function(x) {
    if(x == "repo_name") x
    else paste(x, "_", col_suffix, sep="")
  })
  
  # Join tables
  join_tbl(join_data)
  
}

# Do the work
process_code_chunk_freq(table_code_chunk_freq_10_50, "10_50")
process_code_chunk_freq(table_code_chunk_freq_5_80, "5_80")

```

```{r lang_bytes_by_repo}
# Import table of language bytes by repo
lang_bytes_by_repo <- list_tabledata(project = proj, dataset = ds_analysis, table = table_lang_bytes_by_repo) 

# Calculate total bytes per repo not including data file types
total_bytes_by_repo_no_data <- lang_bytes_by_repo %>% 
  filter(!language %in% non_lang_file_types) %>%
  group_by(repo_name) %>%
  summarise(total_bytes_no_data = sum(total_bytes))

# Add analysis for each top language
for (lang in top_langs) {
  
  # Column names to add to repo data
  colname_exists <- paste("includes_", lang, sep = "")
  colname_bytes <- paste("bytes_", lang, sep = "")
  colname_pct <- paste("pct_bytes_no_data_", lang, sep = "")
  
  # Filter language bytes table for the language
  bytes_this_lang <- lang_bytes_by_repo %>% group_by(repo_name) %>% filter(language == lang)
  
  # Whether the repo includes the language or not
  repo_data[[colname_exists]] <- sapply(repo_data$repo_name, function(x) x %in% bytes_this_lang$repo_name)
  
  # Number of bytes of the language
  repo_data[[colname_bytes]] <- 
    sapply(repo_data$repo_name, 
           function(x) {
             a <- unlist(bytes_this_lang[which(bytes_this_lang$repo_name == x), "total_bytes"])
             if(length(a) > 0) a 
             else 0
           })
  
  # Percentage of bytes in this language not including data file types
  repo_data[[colname_pct]] <- 
    sapply(repo_data$repo_name, 
           function(x) {
             a <- unlist(bytes_this_lang[which(bytes_this_lang$repo_name == x), "total_bytes"])
             if(length(a) > 0) {
               t <- unlist(total_bytes_by_repo_no_data[which(total_bytes_by_repo_no_data$repo_name == x), "total_bytes_no_data"])
               a / t
             }
             else 0
           })
}

rm(bytes_this_lang, colname_bytes, colname_exists, colname_pct, total_bytes_by_repo_no_data, lang_bytes_by_repo)
```

```{r test_cases}
# Query to join file size to test case table
query <- paste("SELECT
  test_cases.repo_name AS repo_name,
  test_cases.path AS path,
  test_cases.language AS language,
  test_cases.lines AS lines,
  file_info.size AS size
FROM (
  SELECT
    *
  FROM ", table_str(ds_analysis, table_test_cases), ") AS test_cases
INNER JOIN (
  SELECT
    sha,
    size
  FROM
    ", table_str(ds_gh, table_file_info),
"GROUP BY
    sha,
    size ) AS file_info
ON
  test_cases.sha = file_info.sha", sep="")

# Get test cases
test_cases_no_data <- fetch_query_res(query) %>% filter(!language %in% non_lang_file_types)
rm(query)

# Add basic summary of test cases
join_tbl(test_cases_no_data %>% 
           group_by(repo_name) %>% 
           summarise(
             num_test_cases_no_data = n(),
             total_lines_test_cases_no_data = sum(lines),
             total_size_test_cases_no_data = sum(size),
             num_langs_test_cases_no_data = length(unique(language))))

rm(test_cases_no_data)

# Add percentages with respect to whole repo
repo_data$pct_files_test_cases_no_data <- 
  repo_data$num_test_cases_no_data / repo_data$total_files_no_data
repo_data$pct_lang_with_test_cases_no_data <- 
  repo_data$num_langs_test_cases_no_data / repo_data$num_langs_no_data
repo_data$pct_lines_in_test_cases_no_data <-
  repo_data$total_lines_test_cases_no_data / repo_data$total_lines_no_data
repo_data$pct_bytes_in_test_cases_no_data <-
  repo_data$total_size_test_cases_no_data / repo_data$total_file_size_no_data

```




