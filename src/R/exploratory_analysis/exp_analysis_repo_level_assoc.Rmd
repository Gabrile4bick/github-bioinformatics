---
title: "Repo-level associations"
author: "Pamela Russell"
date: "12/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r env_setup}
rm(list=ls())
suppressPackageStartupMessages(library(bigrquery))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(parsedate))
setwd("~/Dropbox/Documents/Github_mining/src/R/exploratory_analysis")
source("project_info.R")
```

```{r util}
# Helpful functions / utilities

# Fetch table from BigQuery
get_table <- function(ds, table) {
  list_tabledata(project = proj, dataset = ds, table = table, max_pages = Inf)
}

# Get formatted table ID e.g. "[github-bioinformatics-171721:analysis.lines_of_code_by_file]"
table_str <- function(dataset, table) {paste("[", proj, ":", dataset, ".", table, "]", sep = "")}

# Get formatted table ID for standard SQL e.g. `github-bioinformatics-171721.repos.file_init_commit`
table_str_std_sql <- function(dataset, table) {paste("`", proj, ".", dataset, ".", table, "`", sep = "")}

# Get query result from BigQuery
fetch_query_res <- function(query) {query_exec(query, project = proj, max_pages = Inf)}

# Join a table to repo_data
join_tbl <- function(data) {
  for (colname in colnames(data)) {
    if (colname != "repo_name" && colname %in% colnames(repo_data)) {
      stop(paste("repo_data already contains column", colname))
    }
  }
  repo_data <<- full_join(repo_data, data, by = "repo_name")
}

# Change NA to 0
na_to_zero <- function(x) {
  if (is.na(x)) 0
  else x
}

# Change NA to false
na_to_false <- function(x) {
  if (is.na(x)) FALSE
  else x
}

# Change NA to zero in repo_data for a bunch of columns
columns_na_to_zero <- function(colnms) {
  for (col in colnms) {
    if (col != "repo_name") {
      repo_data[[col]] <<- sapply(repo_data[[col]], na_to_zero)
    }
  }
}

# Longest contiguous subsequence of true values in a boolean vector
max_contiguous <- function(b) {
  max <- 0
  curr <- 0
  for (i in 1:length(b)) {
    if(b[i]) {curr <- curr + 1} 
    else {
      if (curr > max) {max <- curr}
      curr <- 0
    }
  }
  if (curr > max) {max <- curr}
  max
}

# Max number of consecutive months with data in a specified column
max_consecutive_months <- function(data, repo, month_col, count_true = T) {
  data_repo <- data %>% filter(repo_name == repo)
  total_months <- data_repo[1, "repo_num_months"]
  month_has_data <- rep(F, total_months)
  for (i in 1:nrow(data_repo)) {
    m <- data_repo[i, month_col] + 1
    if (m > length(month_has_data)) {
      warning(paste("Month outside span given in data:", repo, "\n"))
    } else {
      month_has_data[m] <- T
    }
  }
  if (count_true) {
    max_contiguous(month_has_data)
  } else {
    max_contiguous(!month_has_data)
  }
}

# Column names of repo_data matching pattern
cols <- function(p) {
  names(repo_data)[grepl(p, names(repo_data))] 
}

# Add years to datetime
add_years <- function(date, years) {
  d <- as.POSIXlt(date)
  d$year <- d$year + years
  as.Date(d)
}

```

```{r langs_to_use}
# Top languages to include as covariates
# Languages included in at least 50 repos
top_langs <- c("Python",
               "Bourne Shell",
               "R",
               "C/C++ Header",
               "Perl",
               "C++",
               "JavaScript",
               "C",
               "Java",
               "Bourne Again Shell",
               "MATLAB",
               "m4",
               "SQL",
               "Ruby",
               "PHP")

# "Languages" that are actually data file types or web pages
non_lang_file_types <- c("HTML",
                         "JSON",
                         "XML",
                         "CSS",
                         "YAML",
                         "Markdown",
                         "XSD",
                         "DTD",
                         "XSLT",
                         "XMI",
                         "Haml",
                         "XHTML",
                         "XAML")
```

```{r init_df}
# Initialize table for all repo-level data
repo_data <- data.frame(repo_name = character(0))
```

```{r licenses}
join_tbl(
  get_table(ds_gh, table_licenses) %>%
    select(repo_name, license)
)
```

```{r proj_duration}
join_tbl(
  get_table(ds_analysis, table_proj_duration)
)
```

```{r num_devs}
join_tbl(
  get_table(ds_analysis, table_num_devs_by_repo)
)
```

```{r repo_metrics}
join_tbl(
  get_table(ds_gh, table_repo_metrics) %>%
    select(repo_name, is_fork, stargazers_count, watchers_count, forks_count, subscribers_count)
)
```

```{r repo_and_article}
query <- paste("SELECT repo_name, journal, year, use_repo FROM [", 
               proj, ":", ds_lit_search, ".", table_repo_and_article, "]", 
               sep="")
join_tbl(
  query_exec(query, project = proj, max_pages = Inf) %>%
    filter(use_repo == T) %>%
    select(repo_name, journal, year_published = year)
)
```

```{r citations}
article_metadata <- get_table(ds_lit_search, table_article_metadata) %>% 
  select(repo_name, year_pubmed, month_pubmed, day_pubmed, cited_by_pmc, cited_by_pmc_date) %>% 
  rename(num_citations_pmc = cited_by_pmc) %>%
  rename(date_citations = cited_by_pmc_date) %>%
  mutate(date_pubmed = as.Date(paste(year_pubmed, month_pubmed, day_pubmed, sep = "-"))) %>%
  select(repo_name, date_pubmed, num_citations_pmc, date_citations) %>%
  mutate(date_pubmed_plus_2_years = add_years(date_pubmed, 2)) %>% 
  mutate(weeks_since_pub_minus_2_years = difftime(date_citations, date_pubmed_plus_2_years, units = "weeks"))

article_metadata$weeks_since_pub_minus_2_years <- sapply(article_metadata$weeks_since_pub_minus_2_years,
                                                         function(x) {if (x <= 0) NA else x})

article_metadata <- article_metadata %>% 
  mutate(num_citations_per_week_pmc_minus_2_years = num_citations_pmc / weeks_since_pub_minus_2_years) %>% 
  select(repo_name, num_citations_pmc, num_citations_per_week_pmc_minus_2_years)

join_tbl(article_metadata)

```

```{r file_info}
query <- paste("SELECT
  file_info.repo_name AS repo_name,
  loc.language AS language,
  loc.code AS lines_of_code,
  loc.comment as lines_comment,
  file_info.size AS size
FROM (
  SELECT
    repo_name,
    sha
  FROM
    ", table_str(ds_gh, table_file_info), "
  GROUP BY
    repo_name,
    sha,
    size) AS file_info
INNER JOIN (
  SELECT
    sha,
    language,
    comment,
    code
  FROM
    ", table_str(ds_analysis, table_loc), "
  GROUP BY
    sha,
    language,
    comment,
    code) AS loc
ON
  file_info.sha = loc.sha", sep="")

file_info <- query_exec(query, project = proj, max_pages = Inf)
file_info_no_data <- file_info %>% filter(!language %in% non_lang_file_types)

join_tbl(
  file_info %>% 
    group_by(repo_name) %>% 
    summarize(
      num_langs = length(unique(language)),
      total_file_size = sum(as.numeric(size)), 
      largest_file_size = max(as.numeric(size)), 
      mean_file_size = mean(as.numeric(size)))
)

join_tbl(
  file_info_no_data %>% 
    group_by(repo_name) %>% 
    summarize(
      num_langs_no_data = length(unique(language)),
      total_file_size_no_data = sum(as.numeric(size)), 
      largest_file_size_no_data = max(as.numeric(size)), 
      mean_file_size_no_data = mean(as.numeric(size)))
)

```

```{r file_init_commit}
# Query to get summary of times when files were first committed
query <- paste("
SELECT
  COUNT(repo_name) AS num_files_same_commit_hour,
  repo_name,
  repo_duration_months + 1 as repo_num_months,
  file_commit_months_from_proj_start,
  file_commit_days_from_proj_start,
  file_commit_hours_from_proj_start
FROM (
  SELECT
    project_duration.repo_name AS repo_name,
    project_duration.first_commit AS repo_first_commit,
    project_duration.last_commit AS repo_last_commit,
    DATETIME_DIFF(DATETIME(TIMESTAMP(project_duration.last_commit)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      HOUR) AS repo_duration_hours,
    DATETIME_DIFF(DATETIME(TIMESTAMP(project_duration.last_commit)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      DAY) AS repo_duration_days,
    DATETIME_DIFF(DATETIME(TIMESTAMP(project_duration.last_commit)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      MONTH) AS repo_duration_months,
    init_commit.init_commit_timestamp AS file_commit,
    DATETIME_DIFF(DATETIME(TIMESTAMP(init_commit.init_commit_timestamp)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      HOUR) AS file_commit_hours_from_proj_start,
    DATETIME_DIFF(DATETIME(TIMESTAMP(init_commit.init_commit_timestamp)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      DAY) AS file_commit_days_from_proj_start,
    DATETIME_DIFF(DATETIME(TIMESTAMP(init_commit.init_commit_timestamp)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      MONTH) AS file_commit_months_from_proj_start
  FROM (
    SELECT
      repo_name,
      init_commit_timestamp
    FROM
      ", table_str_std_sql(ds_gh, table_init_commit), ") AS init_commit
  INNER JOIN
    ", table_str_std_sql(ds_analysis, table_proj_duration), " AS project_duration
  ON
    init_commit.repo_name = project_duration.repo_name)
GROUP BY
  repo_name,
  repo_duration_hours,
  repo_duration_days,
  repo_duration_months,
  file_commit_hours_from_proj_start,
  file_commit_days_from_proj_start,
  file_commit_months_from_proj_start
ORDER BY
  repo_name",
sep = "")

# Pull down query results
init_commit <- query_exec(query, project = proj, max_pages = Inf, use_legacy_sql = F)

# Number of different days when new files were added
join_tbl(
  init_commit %>% 
    select(repo_name, file_commit_days_from_proj_start) %>% 
    group_by(repo_name) %>% 
    summarise(num_days_new_files_added = n_distinct(file_commit_days_from_proj_start))
)

# Mean number of files added per day when new files were added
join_tbl(
  init_commit %>% 
    group_by(repo_name, file_commit_days_from_proj_start) %>% 
    summarise(files_added = sum(num_files_same_commit_hour)) %>% 
    summarise(mean_new_files_per_day_with_new_files = mean(files_added))
)

# Mean day since project initial commit that new files were added
join_tbl(
  init_commit %>% 
    group_by(repo_name) %>% 
    summarise(mean_day_new_files_added = sum(num_files_same_commit_hour * file_commit_days_from_proj_start) / sum(num_files_same_commit_hour))
)

# Mean number of files added per month including months with no new files
join_tbl(
  init_commit %>% 
    group_by(repo_name) %>% 
    summarize(mean_files_added_per_month = sum(num_files_same_commit_hour / repo_num_months))
)

# Proportion of months with new files added
join_tbl(
  init_commit %>% 
    group_by(repo_name) %>% 
    summarise(months_with_new_files = n_distinct(file_commit_months_from_proj_start)) %>% 
    inner_join(init_commit %>% 
                 select(repo_name, repo_num_months) %>% 
                 distinct(), by = "repo_name") %>% 
    mutate(pct_months_new_files_added = months_with_new_files / repo_num_months) %>% 
    select(repo_name, pct_months_new_files_added)
)

# Longest number of months in a row with new files added
new_file_consecutive_months <- init_commit %>% 
  select(repo_name) %>% 
  distinct()
new_file_consecutive_months$consecutive_months_with_new_files_added <- 
  sapply(new_file_consecutive_months$repo_name, function(repo) max_consecutive_months(init_commit, repo, "file_commit_months_from_proj_start"))
new_file_consecutive_months$consecutive_months_no_new_files_added <- 
  sapply(new_file_consecutive_months$repo_name, function(repo) max_consecutive_months(init_commit, repo, "file_commit_months_from_proj_start", F))
join_tbl(new_file_consecutive_months)

rm(init_commit, new_file_consecutive_months)

```

```{r commit_types}
# Add data from commit types table
commit_types <- get_table(ds_analysis, table_commit_types) 
commit_types <- commit_types %>%
  mutate(num_bug_fix_commits = sapply(commit_types$num_bug_fix_commits, na_to_zero)) %>%
  mutate(pct_bug_fix_commits = num_bug_fix_commits / num_commits)
join_tbl(commit_types)
rm(commit_types)
```

```{r commits}
commits_table_str <- table_str(ds_gh, table_commits)

# Percentage of commits with different author and committer
query <- paste("
SELECT
  num_commits.repo_name AS repo_name,
  num_commits_diff_author.num_commits / num_commits.num_commits as pct_commits_diff_author_committer
FROM (
  SELECT
    repo_name,
    COUNT(repo_name) AS num_commits
  FROM
    ", commits_table_str, "
  GROUP BY
    repo_name) AS num_commits
INNER JOIN (
  SELECT
    repo_name,
    COUNT(repo_name) AS num_commits
  FROM
    ", commits_table_str, "
  WHERE
    author_id != committer_id
  GROUP BY
    repo_name) AS num_commits_diff_author
ON
  num_commits.repo_name = num_commits_diff_author.repo_name
", sep = "")

join_tbl(fetch_query_res(query))
repo_data$pct_commits_diff_author_committer <- sapply(repo_data$pct_commits_diff_author_committer, na_to_zero)

# Median and mean commit message length
query <- paste("
SELECT
  repo_name,
  NTH(501, QUANTILES(LENGTH(commit_message), 1001)) AS median_commit_message_len,
  AVG(LENGTH(commit_message)) AS mean_commit_message_len
FROM
  ", commits_table_str, "
GROUP BY
  repo_name
", sep = "")
join_tbl(fetch_query_res(query))

# Commit timing by month
query <- paste("
SELECT
  COUNT(repo_name) AS num_commits_same_month,
  repo_name,
  DATETIME_DIFF(DATETIME(TIMESTAMP(repo_last_commit)),
    DATETIME(TIMESTAMP(repo_first_commit)),
    MONTH) + 1 AS repo_num_months,
  DATETIME_DIFF(DATETIME(TIMESTAMP(commit_timestamp)),
    DATETIME(TIMESTAMP(repo_first_commit)),
    MONTH) AS commit_months_from_proj_start
FROM (
  SELECT
    first_last.repo_name AS repo_name,
    first_last.first_commit AS repo_first_commit,
    first_last.last_commit AS repo_last_commit,
    commits.author_commit_date AS commit_timestamp
  FROM (
    SELECT
      repo_name,
      author_commit_date
    FROM
      ", table_str_std_sql(ds_gh, table_commits), ") AS commits
  INNER JOIN (
    SELECT
      repo_name,
      first_commit,
      last_commit
    FROM (
      SELECT
        repo_name,
        MIN(author_commit_date) AS first_commit,
        MAX(author_commit_date) AS last_commit
      FROM
        ", table_str_std_sql(ds_gh, table_commits), "
      GROUP BY
        repo_name ) ) AS first_last
  ON
    commits.repo_name = first_last.repo_name )
GROUP BY
  repo_name,
  repo_num_months,
  commit_months_from_proj_start
ORDER BY
  repo_name,
  commit_months_from_proj_start
", sep = "")

commit_months <- query_exec(query, project = proj, max_pages = Inf, use_legacy_sql = F)

# Mean commits per month including months with no commits
join_tbl(
  commit_months %>% 
    group_by(repo_name) %>% 
    summarize(mean_commits_per_month = sum(num_commits_same_month / repo_num_months))
)

# Percentage of months with at least one commit
join_tbl(
  commit_months %>% 
    group_by(repo_name) %>% 
    summarise(months_with_commits = n_distinct(commit_months_from_proj_start)) %>% 
    inner_join(commit_months %>% 
                 select(repo_name, repo_num_months) %>% 
                 distinct(), by = "repo_name") %>% 
    mutate(pct_months_with_commits = months_with_commits / repo_num_months) %>% 
    select(repo_name, pct_months_with_commits)
)

# Longest number of months in a row with commits
commit_consecutive_months <- commit_months %>% 
  select(repo_name) %>% 
  distinct()
commit_consecutive_months$consecutive_months_with_commits <- 
  sapply(commit_consecutive_months$repo_name, function(repo) max_consecutive_months(commit_months, repo, "commit_months_from_proj_start"))
commit_consecutive_months$consecutive_months_no_commits <- 
  sapply(commit_consecutive_months$repo_name, function(repo) max_consecutive_months(commit_months, repo, "commit_months_from_proj_start", F))
join_tbl(commit_consecutive_months)

rm(commits_table_str, commit_months, commit_consecutive_months)
```

```{r code_chunk_freq}
# Function to process one version of code chunk frequency table
process_code_chunk_freq <- function(table_name, col_suffix) {
  
  query <- paste("SELECT repo_name, num_occurrences FROM [", 
                 proj, ":", ds_analysis, ".", table_name, "]", 
                 sep = "")
  
  code_chunk_freq <- query_exec(query, project = proj, max_pages = Inf)
  
  total_code_chunks <- code_chunk_freq %>% 
    group_by(repo_name) %>% 
    summarize(total_code_chunks = n())
  
  repeated_code_chunks <- code_chunk_freq %>% 
    filter(num_occurrences > 1) %>%
    group_by(repo_name) %>% 
    summarize(repeated_code_chunks = n())
  
  max_code_chunk_occurrence <- code_chunk_freq %>%
    group_by(repo_name) %>%
    summarise(max_code_chunk_occurrence = max(num_occurrences))
  
  mean_code_chunk_occurrence <- code_chunk_freq %>%
    group_by(repo_name) %>%
    summarise(mean_code_chunk_occurrence = mean(num_occurrences))
  
  join_data <- total_code_chunks %>%
    full_join(repeated_code_chunks, by = "repo_name") %>%
    full_join(max_code_chunk_occurrence, by = "repo_name") %>%
    full_join(mean_code_chunk_occurrence, by = "repo_name")
  
  colnames(join_data) <- sapply(colnames(join_data), function(x) {
    if(x == "repo_name") x
    else paste(x, "_", col_suffix, sep="")
  })
  
  # Join tables
  join_tbl(join_data)
  
}

# Do the work
process_code_chunk_freq(table_code_chunk_freq_10_50, "10_50")
process_code_chunk_freq(table_code_chunk_freq_5_80, "5_80")

```

```{r lang_bytes_by_repo}
# Import table of language bytes by repo
lang_bytes_by_repo <- get_table(ds_analysis, table_lang_bytes_by_repo) 

# Calculate total bytes per repo not including data file types
total_bytes_by_repo_no_data <- lang_bytes_by_repo %>% 
  filter(!language %in% non_lang_file_types) %>%
  group_by(repo_name) %>%
  summarise(total_bytes_no_data = sum(total_bytes))

# Add analysis for each top language
for (lang in top_langs) {
  
  # Column names to add to repo data
  colname_exists <- paste("includes_", lang, sep = "")
  colname_bytes <- paste("bytes_", lang, sep = "")
  colname_pct <- paste("pct_bytes_no_data_", lang, sep = "")
  
  # Filter language bytes table for the language
  bytes_this_lang <- lang_bytes_by_repo %>% group_by(repo_name) %>% filter(language == lang)
  
  # Whether the repo includes the language or not
  repo_data[[colname_exists]] <- sapply(repo_data$repo_name, function(x) x %in% bytes_this_lang$repo_name)
  
  # Number of bytes of the language
  repo_data[[colname_bytes]] <- 
    sapply(repo_data$repo_name, 
           function(x) {
             a <- unlist(bytes_this_lang[which(bytes_this_lang$repo_name == x), "total_bytes"])
             if(length(a) > 0) a 
             else 0
           })
  
  # Percentage of bytes in this language not including data file types
  repo_data[[colname_pct]] <- 
    sapply(repo_data$repo_name, 
           function(x) {
             a <- unlist(bytes_this_lang[which(bytes_this_lang$repo_name == x), "total_bytes"])
             if(length(a) > 0) {
               t <- unlist(total_bytes_by_repo_no_data[which(total_bytes_by_repo_no_data$repo_name == x), "total_bytes_no_data"])
               a / t
             }
             else 0
           })
}

rm(bytes_this_lang, colname_bytes, colname_exists, colname_pct, total_bytes_by_repo_no_data, lang_bytes_by_repo)
```

```{r lines_of_code_by_file}
# Function to get various lines of code measurements
loc_data <- function(file_info_df, suffix) {
  data <- file_info_df %>% 
    group_by(repo_name) %>% 
    summarise(
      num_files = n(),
      mean_bytes = mean(size),
      total_lines_of_code = sum(lines_of_code), 
      total_lines_comment = sum(lines_comment), 
      total_lines_code_and_comment = sum(lines_of_code) + sum(lines_comment),
      max_lines_code = max(lines_of_code),
      max_lines_code_and_comment = max(lines_of_code + lines_comment),
      mean_lines_code = mean(lines_of_code),
      mean_lines_code_and_comment = mean(lines_of_code + lines_comment),
      mean_bytes_per_line_code_and_comment = sum(size) / (sum(lines_of_code) + sum(lines_comment))
    ) %>% 
    mutate(pct_lines_comment = total_lines_comment / total_lines_code_and_comment)
  
  colnames(data) <- sapply(colnames(data), function(x) {
    if (x == "repo_name") x
    else paste(x, suffix, sep = "")
  })
  
  data
}

loc_data_file_info <- loc_data(file_info, "")
join_tbl(loc_data_file_info)
columns_na_to_zero(colnames(loc_data_file_info))
rm(loc_data_file_info)

loc_data_file_info_no_data <- loc_data(file_info_no_data, "_no_data")
join_tbl(loc_data_file_info_no_data)
columns_na_to_zero(colnames(loc_data_file_info_no_data))
rm(loc_data_file_info_no_data)

for (lang in top_langs) {
  data <- loc_data(
    file_info %>% filter(language == lang), 
    paste("_", lang, sep = ""))
  join_tbl(data)
  columns_na_to_zero(colnames(data))
  rm(data)
}

```

```{r test_cases}
# Query to join file size to test case table
query <- paste("SELECT
  test_cases.repo_name AS repo_name,
  test_cases.path AS path,
  test_cases.language AS language,
  test_cases.lines AS lines,
  file_info.size AS size
FROM (
  SELECT
    *
  FROM ", table_str(ds_analysis, table_test_cases), ") AS test_cases
INNER JOIN (
  SELECT
    sha,
    size
  FROM
    ", table_str(ds_gh, table_file_info),
"GROUP BY
    sha,
    size ) AS file_info
ON
  test_cases.sha = file_info.sha", sep="")

# Get test cases
test_cases_no_data <- fetch_query_res(query) %>% filter(!language %in% non_lang_file_types)
rm(query)

# Add basic summary of test cases
join_tbl(test_cases_no_data %>% 
           group_by(repo_name) %>% 
           summarise(
             num_test_cases_no_data = n(),
             total_lines_test_cases_no_data = sum(lines),
             total_size_test_cases_no_data = sum(size),
             num_langs_test_cases_no_data = length(unique(language))))
repo_data$total_lines_test_cases_no_data <- sapply(repo_data$total_lines_test_cases_no_data, na_to_zero)
repo_data$total_size_test_cases_no_data <- sapply(repo_data$total_size_test_cases_no_data, na_to_zero)
repo_data$num_langs_test_cases_no_data <- sapply(repo_data$num_langs_test_cases_no_data, na_to_zero)

# Add percentages with respect to whole repo
repo_data$pct_files_test_cases_no_data <- 
  repo_data$num_test_cases_no_data / repo_data$num_files_no_data
repo_data$pct_lang_with_test_cases_no_data <- 
  repo_data$num_langs_test_cases_no_data / repo_data$num_langs_no_data
repo_data$pct_lines_in_test_cases_no_data <-
  repo_data$total_lines_test_cases_no_data / repo_data$total_lines_of_code_no_data
repo_data$pct_bytes_in_test_cases_no_data <-
  repo_data$total_size_test_cases_no_data / repo_data$total_file_size_no_data

```

```{r language_properties}
# Import language properties
lang_exec_method <- get_table(ds_lang, table_exec_method)
lang_paradigm <- get_table(ds_lang, table_paradigm)
lang_type_system <- get_table(ds_lang, table_type_system)
lang_type_system_bool <- as.tbl(lang_type_system) %>% 
  mutate(type_system_static = system == "static", 
         type_system_dynamic = system == "dynamic", 
         type_system_safe = safety == "safe", 
         type_system_unsafe = safety == "unsafe", 
         compatibility_nominative = compatibility == "nominative", 
         compatibility_structural = compatibility == "structural", 
         compatibility_duck = compatibility == "duck")

# Join language properties to file info
file_info_with_lang_no_data <- as.tbl(file_info_no_data) %>% 
  mutate(language = tolower(language)) %>% 
  select(repo_name, language, size) %>% 
  left_join(lang_exec_method, by = "language") %>% 
  left_join(lang_type_system, by = "language") %>% 
  left_join(lang_paradigm, by = "language") %>%
  mutate(type_system_static = system == "static") %>%
  mutate(type_system_dynamic = system == "dynamic") %>%
  mutate(type_system_safe = safety == "safe") %>%
  mutate(type_system_unsafe = safety == "unsafe") %>%
  mutate(compatibility_nominative = compatibility == "nominative") %>%
  mutate(compatibility_structural = compatibility == "structural") %>%
  mutate(compatibility_duck = compatibility == "duck")

# Function to count bytes and files for a boolean column in the file info
add_file_counts_bool <- function(col) {
  filtered <- file_info_with_lang_no_data[which(file_info_with_lang_no_data[[col]]), ]
  
  # Total file size
  bytes <- filtered %>% 
    group_by(repo_name) %>% 
    summarise(xxxxx = sum(size)) %>% 
    left_join(repo_data %>% 
                select(repo_name, total_file_size_no_data), by = "repo_name") %>% 
    mutate(yyyyy = xxxxx / total_file_size_no_data) %>% 
    mutate(zzzzz = xxxxx > 0) %>%
    select(repo_name, xxxxx, yyyyy, zzzzz)
  colname_bytes <- paste("total_bytes_no_data_", col, sep = "")
  colname_pct <- paste("pct_bytes_no_data_", col, sep = "")
  colname_bool <- paste("has_", col, sep = "")
  colnames(bytes) <- c("repo_name", colname_bytes, colname_pct, colname_bool)
  join_tbl(bytes)
  repo_data[[colname_bytes]] <<- sapply(repo_data[[colname_bytes]], na_to_zero)
  repo_data[[colname_pct]] <<- sapply(repo_data[[colname_pct]], na_to_zero)
  repo_data[[colname_bool]] <<- sapply(repo_data[[colname_bool]], na_to_false)
  
  # Total number of files
  files <- filtered %>% 
    group_by(repo_name) %>% 
    summarise(xxxxx = n()) %>%
    left_join(repo_data %>%
                select(repo_name, num_files_no_data), by = "repo_name") %>%
    mutate(yyyyy = xxxxx / num_files_no_data) %>%
    select(repo_name, xxxxx, yyyyy)
  colname_files <- paste("total_files_", col, sep = "")
  colname_pct <- paste("pct_files_no_data_", col, sep = "")
  colnames(files) <- c("repo_name", colname_files, colname_pct)
  join_tbl(files)
  repo_data[[colname_files]] <<- sapply(repo_data[[colname_files]], na_to_zero)
  repo_data[[colname_pct]] <<- sapply(repo_data[[colname_pct]], na_to_zero)
  
}

add_file_counts_bool("interpreted")
add_file_counts_bool("compiled")
add_file_counts_bool("type_system_static")
add_file_counts_bool("type_system_dynamic")
add_file_counts_bool("type_system_safe")
add_file_counts_bool("type_system_unsafe")
add_file_counts_bool("array")
add_file_counts_bool("declarative")
add_file_counts_bool("functional_impure")
add_file_counts_bool("functional_pure")
add_file_counts_bool("imperative")
add_file_counts_bool("logic")
add_file_counts_bool("object_oriented")
add_file_counts_bool("procedural")
add_file_counts_bool("compatibility_nominative")
add_file_counts_bool("compatibility_structural")
add_file_counts_bool("compatibility_duck")


# Amount of code in test cases for various language properties
test_cases_by_lang_property <- as.tbl(test_cases_no_data) %>% 
  mutate(language = tolower(language)) %>% 
  left_join(lang_exec_method, by = "language") %>%
  left_join(lang_paradigm, by = "language") %>%
  left_join(lang_type_system_bool, by = "language")

# Function to count amount of code in test cases for files with a boolean property
add_test_case_code_amt_bool <- function(colnm) {
  filtered <- test_cases_by_lang_property[which(test_cases_by_lang_property[[colnm]]),]
  summarized <- filtered %>% 
    group_by(repo_name) %>% 
    summarize(total_bytes_test_cases_category = sum(size))
  joined <- repo_data[, c("repo_name", paste("total_bytes_no_data_", colnm, sep = ""))] %>% 
    left_join(summarized, by = "repo_name")
  joined$total_bytes_test_cases_category <- sapply(joined$total_bytes_test_cases_category, na_to_zero)
  colnames(joined) <- c("repo_name", "total_bytes_no_data_category", "total_bytes_test_cases_category")
  joined <- joined %>% 
    mutate(pct_bytes_test_cases_category = total_bytes_test_cases_category / total_bytes_no_data_category) %>%
    select(repo_name, total_bytes_test_cases_category, pct_bytes_test_cases_category)
  colnames(joined) <- sapply(colnames(joined), function(x) sub("category", colnm, x))
  join_tbl(joined)
}

add_test_case_code_amt_bool("interpreted")
add_test_case_code_amt_bool("compiled")
add_test_case_code_amt_bool("type_system_static")
add_test_case_code_amt_bool("type_system_dynamic")
add_test_case_code_amt_bool("type_system_safe")
add_test_case_code_amt_bool("type_system_unsafe")
add_test_case_code_amt_bool("array")
add_test_case_code_amt_bool("declarative")
add_test_case_code_amt_bool("functional_impure")
add_test_case_code_amt_bool("functional_pure")
add_test_case_code_amt_bool("imperative")
add_test_case_code_amt_bool("logic")
add_test_case_code_amt_bool("object_oriented")
add_test_case_code_amt_bool("procedural")
add_test_case_code_amt_bool("compatibility_nominative")
add_test_case_code_amt_bool("compatibility_structural")
add_test_case_code_amt_bool("compatibility_duck")

rm(test_cases_no_data, test_cases_by_lang_property)

```

```{r non_committer_contributions}
commits <- fetch_query_res(paste("
SELECT
  repo_name,
  committer_id,
  author_id
FROM
  ", table_str(ds_gh, table_commits), "
GROUP BY
  repo_name,
  committer_id,
  author_id"))

# Number of commit authors who are never committers
join_tbl(
  as.tbl(commits) %>% 
    select(repo_name, author_id) %>% 
    anti_join(commits %>% select(repo_name, committer_id), by = c("repo_name", "author_id" = "committer_id")) %>% 
    group_by(repo_name) %>% 
    summarise(num_non_committing_authors = n())
)

repo_data$num_non_committing_authors <- sapply(repo_data$num_non_committing_authors, na_to_zero)
rm(commits)
```

```{r paper_topics}
# Run topic modeling of article abstracts
source("exp_analysis_topics.R")
# Add each topic as a binary variable
for(t in unique(abstract_top_topics$topic)) {
  colname <- paste("topic_", t, sep = "")
  df <- abstract_top_topics %>% filter(topic == t) %>% select(repo_name)
  df[[colname]] <- TRUE
  join_tbl(df)
  repo_data[[colname]] <- sapply(repo_data[[colname]], na_to_false)
}
# Number of repos per topic
num_repos_per_topic
rm(abstract_top_topics, article_data, article_metadata, df, dtm, lda, num_repos_per_topic, topics, topics_specialized)
```

```{r associations}
k <- ncol(repo_data)
n_comparisons <- k * (k - 1)
pval_cutoff <- 0.0001 / n_comparisons

assoc.numeric <- data.frame(var1 = character(), var2 = character(), corr = numeric(), pval = numeric())
assoc.mannwhitney <- data.frame(var1 = character(), var2 = character(), pval = numeric())
assoc.sets <- data.frame(var1 = character(), var2 = character(), similarity = numeric())

add_mw <- function(data_numeric, data_logical, name_numeric, name_logical) {
  x <- tryCatch({
    data_yes <- data_numeric[which(data_logical)]
    data_no <- data_numeric[which(!data_logical)]
    res <- wilcox.test(data_yes, data_no, alternative = "two.sided", paired = F)
    if(res$p.value < pval_cutoff) {
      record <- data.frame(var1 = name_numeric, var2 = name_logical, pval = res$p.value)
      assoc.mannwhitney <<- rbind(assoc.mannwhitney, record)
    }
  }, error = function(e) {
    message(paste("CAUGHT ERROR on", name_numeric, "and", name_logical))
  })
}

for (i in 1:k) {
  data1 <- repo_data[[i]]
  name1 <- names(repo_data)[i]
  if(!is.numeric(data1) && !is.logical(data1)) {
    message(paste("Skipping", name1))
  }
  for (j in 1:k) {
    data2 <- repo_data[[j]]
    name2 <- names(repo_data)[j]
    
    # Only compare each pair of columns once
    if (i < j) {
      
      if (is.numeric(data1) && is.numeric(data2)) {
        # Both numeric
        x <- tryCatch({
          res <- cor.test(data1, data2)
          if (res$estimate > 0.5 && res$p.value < pval_cutoff) {
            record <- data.frame(var1 = name1, var2 = name2, corr = res$estimate, pval = res$p.value)
            assoc.numeric <- rbind(assoc.numeric, record)
          }
        }, error = function(e) {
          message(paste("CAUGHT ERROR on", name1, "and", name2))
        })
      } else if (is.numeric(data1) && is.logical(data2)) {
        # data1 numeric, data2 logical
        add_mw(data1, data2, name1, name2)
      } else if (is.logical(data1) && is.numeric(data2)) {
        # data1 logical, data2 numeric
        add_mw(data2, data1, name2, name1)
      } else if (is.logical(data1) && is.logical(data2)) {
        # both logical
        repos1 <- repo_data[which(data1), "repo_name"]
        repos2 <- repo_data[which(data2), "repo_name"]
        jaccard <- length(intersect(repos1, repos2)) / length(union(repos1, repos2))
        record <- data.frame(var1 = name1, var2 = name2, similarity = jaccard)
        assoc.sets <- rbind(assoc.sets, record)
      }
    }
  }
}
rm(data1, data2, i, j, k, n_comparisons, name1, name2, res, x, record,
   pval_cutoff, jaccard, repos1, repos2)
```

```{r save_data}
# Save repo_data to a file
repo_data_cached <- repo_data
save(repo_data_cached, file = repo_data_file)
```

