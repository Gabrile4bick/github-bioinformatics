---
title: "Repo-level associations"
author: "Pamela Russell"
date: "12/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r env_setup}
rm(list=ls())
suppressPackageStartupMessages(library(bigrquery))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(parsedate))
setwd("~/Dropbox/Documents/Github_mining/src/R/exploratory_analysis")
source("project_info.R")
```

```{r util}
# Helpful functions / utilities

# Fetch table from BigQuery
get_table <- function(ds, table) {
  list_tabledata(project = proj, dataset = ds, table = table, max_pages = Inf)
}

# Get formatted table ID e.g. "[github-bioinformatics-171721:analysis.lines_of_code_by_file]"
table_str <- function(dataset, table) {paste("[", proj, ":", dataset, ".", table, "]", sep = "")}

# Get formatted table ID for standard SQL e.g. `github-bioinformatics-171721.repos.file_init_commit`
table_str_std_sql <- function(dataset, table) {paste("`", proj, ".", dataset, ".", table, "`", sep = "")}

# Get query result from BigQuery
fetch_query_res <- function(query) {query_exec(query, project = proj, max_pages = Inf)}

# Join a table to repo_data
join_tbl <- function(data) {
  repo_data <<- full_join(repo_data, data, by = "repo_name")
}

# Change NA to 0
na_to_zero <- function(x) {
  if (is.na(x)) 0
  else x
}

# Longest contiguous subsequence of true values in a boolean vector
max_contiguous <- function(b) {
  max <- 0
  curr <- 0
  for (i in 1:length(b)) {
    if(b[i]) {curr <- curr + 1} 
    else {
      if (curr > max) {max <- curr}
      curr <- 0
    }
  }
  if (curr > max) {max <- curr}
  max
}
```

```{r langs_to_use}
# Top languages to include as covariates
# Languages included in at least 50 repos
top_langs <- c("Python",
               "Bourne Shell",
               "R",
               "C/C++ Header",
               "Perl",
               "C++",
               "JavaScript",
               "C",
               "Java",
               "Bourne Again Shell",
               "MATLAB",
               "m4",
               "SQL",
               "Ruby",
               "PHP")

# "Languages" that are actually data file types or web pages
non_lang_file_types <- c("HTML",
                         "JSON",
                         "XML",
                         "CSS",
                         "YAML",
                         "Markdown",
                         "XSD",
                         "DTD",
                         "XSLT",
                         "XMI",
                         "Haml",
                         "XHTML",
                         "XAML")
```

```{r init_df}
# Initialize table for all repo-level data
repo_data <- data.frame(repo_name = character(0))
```

```{r licenses}
join_tbl(
  get_table(ds_gh, table_licenses) %>%
    select(repo_name, license)
)
```

```{r proj_duration}
join_tbl(
  get_table(ds_analysis, table_proj_duration)
)
```

```{r num_devs}
join_tbl(
  get_table(ds_analysis, table_num_devs_by_repo)
)
```

```{r repo_metrics}
join_tbl(
  get_table(ds_gh, table_repo_metrics) %>%
    select(repo_name, is_fork, stargazers_count, watchers_count, forks_count, subscribers_count)
)
```

```{r repo_and_article}
query <- paste("SELECT repo_name, journal, year, use_repo FROM [", 
               proj, ":", ds_lit_search, ".", table_repo_and_article, "]", 
               sep="")
join_tbl(
  query_exec(query, project = proj) %>%
    filter(use_repo == T) %>%
    select(repo_name, journal, year_published = year)
)
```

```{r file_info}
query <- paste("SELECT
  file_info.repo_name AS repo_name,
  loc.language AS language,
  loc.code AS lines_of_code,
  loc.comment as lines_comment,
  file_info.size AS size
FROM (
  SELECT
    repo_name,
    sha
  FROM
    ", table_str(ds_gh, table_file_info), "
  GROUP BY
    repo_name,
    sha,
    size) AS file_info
INNER JOIN (
  SELECT
    sha,
    language,
    comment,
    code
  FROM
    ", table_str(ds_analysis, table_loc), "
  GROUP BY
    sha,
    language,
    comment,
    code) AS loc
ON
  file_info.sha = loc.sha", sep="")

file_info <- query_exec(query, project = proj, max_pages = Inf)
file_info_no_data <- file_info %>% filter(!language %in% non_lang_file_types)

join_tbl(
  file_info %>% 
    group_by(repo_name) %>% 
    summarize(
      num_langs = length(unique(language)),
      total_file_size = sum(as.numeric(size)), 
      largest_file_size = max(as.numeric(size)), 
      mean_file_size = mean(as.numeric(size)))
)

join_tbl(
  file_info_no_data %>% 
    group_by(repo_name) %>% 
    summarize(
      num_langs_no_data = length(unique(language)),
      total_file_size_no_data = sum(as.numeric(size)), 
      largest_file_size_no_data = max(as.numeric(size)), 
      mean_file_size_no_data = mean(as.numeric(size)))
)

```

```{r file_init_commit}
# Query to get summary of times when files were first committed
query <- paste("
SELECT
  COUNT(repo_name) AS num_files_same_commit_hour,
  repo_name,
  repo_duration_months + 1 as repo_num_months,
  file_commit_months_from_proj_start,
  file_commit_days_from_proj_start,
  file_commit_hours_from_proj_start
FROM (
  SELECT
    project_duration.repo_name AS repo_name,
    project_duration.first_commit AS repo_first_commit,
    project_duration.last_commit AS repo_last_commit,
    DATETIME_DIFF(DATETIME(TIMESTAMP(project_duration.last_commit)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      HOUR) AS repo_duration_hours,
    DATETIME_DIFF(DATETIME(TIMESTAMP(project_duration.last_commit)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      DAY) AS repo_duration_days,
    DATETIME_DIFF(DATETIME(TIMESTAMP(project_duration.last_commit)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      MONTH) AS repo_duration_months,
    init_commit.init_commit_timestamp AS file_commit,
    DATETIME_DIFF(DATETIME(TIMESTAMP(init_commit.init_commit_timestamp)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      HOUR) AS file_commit_hours_from_proj_start,
    DATETIME_DIFF(DATETIME(TIMESTAMP(init_commit.init_commit_timestamp)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      DAY) AS file_commit_days_from_proj_start,
    DATETIME_DIFF(DATETIME(TIMESTAMP(init_commit.init_commit_timestamp)),
      DATETIME(TIMESTAMP(project_duration.first_commit)),
      MONTH) AS file_commit_months_from_proj_start
  FROM (
    SELECT
      repo_name,
      init_commit_timestamp
    FROM
      ", table_str_std_sql(ds_gh, table_init_commit), ") AS init_commit
  INNER JOIN
    ", table_str_std_sql(ds_analysis, table_proj_duration), " AS project_duration
  ON
    init_commit.repo_name = project_duration.repo_name)
GROUP BY
  repo_name,
  repo_duration_hours,
  repo_duration_days,
  repo_duration_months,
  file_commit_hours_from_proj_start,
  file_commit_days_from_proj_start,
  file_commit_months_from_proj_start
ORDER BY
  repo_name",
sep = "")

# Pull down query results
init_commit <- query_exec(query, project = proj, max_pages = Inf, use_legacy_sql = F)

# Number of different days when new files were added
join_tbl(
  init_commit %>% 
    select(repo_name, file_commit_days_from_proj_start) %>% 
    group_by(repo_name) %>% 
    summarise(num_days_new_files_added = n_distinct(file_commit_days_from_proj_start))
)

# Mean number of files added per day when new files were added
join_tbl(
  init_commit %>% 
    group_by(repo_name, file_commit_days_from_proj_start) %>% 
    summarise(files_added = sum(num_files_same_commit_hour)) %>% 
    summarise(mean_new_files_per_day_with_new_files = mean(files_added))
)

# Mean day since project initial commit that new files were added
join_tbl(
  init_commit %>% 
    group_by(repo_name) %>% 
    summarise(mean_day_new_files_added = sum(num_files_same_commit_hour * file_commit_days_from_proj_start) / sum(num_files_same_commit_hour))
)

# Mean number of files added per month including months with no new files
join_tbl(
  init_commit %>% 
    group_by(repo_name) %>% 
    summarize(mean_files_added_per_month = sum(num_files_same_commit_hour / repo_num_months))
)

# Proportion of months with new files added
join_tbl(
  init_commit %>% 
    group_by(repo_name) %>% 
    summarise(months_with_new_files = n_distinct(file_commit_months_from_proj_start)) %>% 
    inner_join(init_commit %>% 
                 select(repo_name, repo_num_months) %>% 
                 distinct(), by = "repo_name") %>% 
    mutate(pct_months_new_files_added = months_with_new_files / repo_num_months) %>% 
    select(repo_name, pct_months_new_files_added)
)

rm(init_commit)

```

```{r commit_types}
# Add data from commit types table
commit_types <- get_table(ds_analysis, table_commit_types) 
commit_types <- commit_types %>%
  mutate(num_bug_fix_commits = sapply(commit_types$num_bug_fix_commits, na_to_zero)) %>%
  mutate(pct_bug_fix_commits = num_bug_fix_commits / num_commits)
join_tbl(commit_types)
rm(commit_types)
```

```{r commits}
commits_table_str <- table_str(ds_gh, table_commits)

# Percentage of commits with different author and committer
query <- paste("
SELECT
  num_commits.repo_name AS repo_name,
  num_commits_diff_author.num_commits / num_commits.num_commits as pct_commits_diff_author_committer
FROM (
  SELECT
    repo_name,
    COUNT(repo_name) AS num_commits
  FROM
    ", commits_table_str, "
  GROUP BY
    repo_name) AS num_commits
INNER JOIN (
  SELECT
    repo_name,
    COUNT(repo_name) AS num_commits
  FROM
    ", commits_table_str, "
  WHERE
    author_id != committer_id
  GROUP BY
    repo_name) AS num_commits_diff_author
ON
  num_commits.repo_name = num_commits_diff_author.repo_name
", sep = "")

join_tbl(fetch_query_res(query))
repo_data$pct_commits_diff_author_committer <- sapply(repo_data$pct_commits_diff_author_committer, na_to_zero)

# Median and mean commit message length
query <- paste("
SELECT
  repo_name,
  NTH(501, QUANTILES(LENGTH(commit_message), 1001)) AS median_commit_message_len,
  AVG(LENGTH(commit_message)) AS mean_commit_message_len
FROM
  ", commits_table_str, "
GROUP BY
  repo_name
", sep = "")
join_tbl(fetch_query_res(query))

rm(commits_table_str)
```

```{r code_chunk_freq}
# Function to process one version of code chunk frequency table
process_code_chunk_freq <- function(table_name, col_suffix) {
  
  query <- paste("SELECT repo_name, num_occurrences FROM [", 
                 proj, ":", ds_analysis, ".", table_name, "]", 
                 sep = "")
  
  code_chunk_freq <- query_exec(query, project = proj, max_pages = Inf)
  
  total_code_chunks <- code_chunk_freq %>% 
    group_by(repo_name) %>% 
    summarize(total_code_chunks = n())
  
  repeated_code_chunks <- code_chunk_freq %>% 
    filter(num_occurrences > 1) %>%
    group_by(repo_name) %>% 
    summarize(repeated_code_chunks = n())
  
  max_code_chunk_occurrence <- code_chunk_freq %>%
    group_by(repo_name) %>%
    summarise(max_code_chunk_occurrence = max(num_occurrences))
  
  mean_code_chunk_occurrence <- code_chunk_freq %>%
    group_by(repo_name) %>%
    summarise(mean_code_chunk_occurrence = mean(num_occurrences))
  
  join_data <- total_code_chunks %>%
    full_join(repeated_code_chunks, by = "repo_name") %>%
    full_join(max_code_chunk_occurrence, by = "repo_name") %>%
    full_join(mean_code_chunk_occurrence, by = "repo_name")
  
  colnames(join_data) <- sapply(colnames(join_data), function(x) {
    if(x == "repo_name") x
    else paste(x, "_", col_suffix, sep="")
  })
  
  # Join tables
  join_tbl(join_data)
  
}

# Do the work
process_code_chunk_freq(table_code_chunk_freq_10_50, "10_50")
process_code_chunk_freq(table_code_chunk_freq_5_80, "5_80")

```

```{r lang_bytes_by_repo}
# Import table of language bytes by repo
lang_bytes_by_repo <- get_table(ds_analysis, table_lang_bytes_by_repo) 

# Calculate total bytes per repo not including data file types
total_bytes_by_repo_no_data <- lang_bytes_by_repo %>% 
  filter(!language %in% non_lang_file_types) %>%
  group_by(repo_name) %>%
  summarise(total_bytes_no_data = sum(total_bytes))

# Add analysis for each top language
for (lang in top_langs) {
  
  # Column names to add to repo data
  colname_exists <- paste("includes_", lang, sep = "")
  colname_bytes <- paste("bytes_", lang, sep = "")
  colname_pct <- paste("pct_bytes_no_data_", lang, sep = "")
  
  # Filter language bytes table for the language
  bytes_this_lang <- lang_bytes_by_repo %>% group_by(repo_name) %>% filter(language == lang)
  
  # Whether the repo includes the language or not
  repo_data[[colname_exists]] <- sapply(repo_data$repo_name, function(x) x %in% bytes_this_lang$repo_name)
  
  # Number of bytes of the language
  repo_data[[colname_bytes]] <- 
    sapply(repo_data$repo_name, 
           function(x) {
             a <- unlist(bytes_this_lang[which(bytes_this_lang$repo_name == x), "total_bytes"])
             if(length(a) > 0) a 
             else 0
           })
  
  # Percentage of bytes in this language not including data file types
  repo_data[[colname_pct]] <- 
    sapply(repo_data$repo_name, 
           function(x) {
             a <- unlist(bytes_this_lang[which(bytes_this_lang$repo_name == x), "total_bytes"])
             if(length(a) > 0) {
               t <- unlist(total_bytes_by_repo_no_data[which(total_bytes_by_repo_no_data$repo_name == x), "total_bytes_no_data"])
               a / t
             }
             else 0
           })
}

rm(bytes_this_lang, colname_bytes, colname_exists, colname_pct, total_bytes_by_repo_no_data, lang_bytes_by_repo)
```

```{r lines_of_code_by_file}
# Function to get various lines of code measurements
loc_data <- function(file_info_df, suffix) {
  data <- file_info_df %>% 
    group_by(repo_name) %>% 
    summarise(
      num_files = n(),
      total_lines_of_code = sum(lines_of_code), 
      total_lines_comment = sum(lines_comment), 
      total_lines_code_and_comment = sum(lines_of_code) + sum(lines_comment),
      max_lines_code = max(lines_of_code),
      max_lines_code_and_comment = max(lines_of_code + lines_comment),
      mean_lines_code = mean(lines_of_code),
      mean_lines_code_and_comment = mean(lines_of_code + lines_comment),
      mean_bytes_per_line_code_and_comment = sum(size) / (sum(lines_of_code) + sum(lines_comment))
    ) %>% 
    mutate(pct_lines_comment = total_lines_comment / total_lines_code_and_comment)
  
  colnames(data) <- sapply(colnames(data), function(x) {
    if(x == "repo_name") x
    else paste(x, suffix, sep = "")
  })
  
  data
}

join_tbl(loc_data(file_info, ""))
join_tbl(loc_data(file_info_no_data, "_no_data"))

for (lang in top_langs) {
  join_tbl(
    loc_data(
      file_info %>% filter(language == lang), 
      paste("_", lang, sep="")))
}

```

```{r test_cases}
# Query to join file size to test case table
query <- paste("SELECT
  test_cases.repo_name AS repo_name,
  test_cases.path AS path,
  test_cases.language AS language,
  test_cases.lines AS lines,
  file_info.size AS size
FROM (
  SELECT
    *
  FROM ", table_str(ds_analysis, table_test_cases), ") AS test_cases
INNER JOIN (
  SELECT
    sha,
    size
  FROM
    ", table_str(ds_gh, table_file_info),
"GROUP BY
    sha,
    size ) AS file_info
ON
  test_cases.sha = file_info.sha", sep="")

# Get test cases
test_cases_no_data <- fetch_query_res(query) %>% filter(!language %in% non_lang_file_types)
rm(query)

# Add basic summary of test cases
join_tbl(test_cases_no_data %>% 
           group_by(repo_name) %>% 
           summarise(
             num_test_cases_no_data = n(),
             total_lines_test_cases_no_data = sum(lines),
             total_size_test_cases_no_data = sum(size),
             num_langs_test_cases_no_data = length(unique(language))))

rm(test_cases_no_data)

# Add percentages with respect to whole repo
repo_data$pct_files_test_cases_no_data <- 
  repo_data$num_test_cases_no_data / repo_data$num_files_no_data
repo_data$pct_lang_with_test_cases_no_data <- 
  repo_data$num_langs_test_cases_no_data / repo_data$num_langs_no_data
repo_data$pct_lines_in_test_cases_no_data <-
  repo_data$total_lines_test_cases_no_data / repo_data$total_lines_of_code_no_data
repo_data$pct_bytes_in_test_cases_no_data <-
  repo_data$total_size_test_cases_no_data / repo_data$total_file_size_no_data

```




